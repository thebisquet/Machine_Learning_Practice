{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MachineLearning_2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbQZSXdm5HuD",
        "outputId": "8a8a9ff6-c3cc-46ee-c571-e5862647eafc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1.12541308,  1.96429418],\n",
              "       [-1.15329466, -0.50068741],\n",
              "       [ 0.29529406, -0.22809346],\n",
              "       [ 0.57385917, -0.42335076],\n",
              "       [ 1.40955451, -0.81216255]])"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# Load Libraries\n",
        "from sklearn import preprocessing\n",
        "import numpy as np\n",
        "\n",
        "# Create Feature\n",
        "features = np.array([[-100.1, 3240.1],\n",
        "                     [-200.2, -234.1],\n",
        "                     [5000.5, 150.1],\n",
        "                     [6000.6, -125.1],\n",
        "                     [9000.9, -673.1]])\n",
        "\n",
        "# Create Scaler\n",
        "scaler = preprocessing.StandardScaler()\n",
        "\n",
        "# Transform the Feature\n",
        "features_standardized = scaler.fit_transform(features)\n",
        "\n",
        "# Show Feature\n",
        "features_standardized"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # Print Mean and Standard Deviation\n",
        "print(\"Mean:\", round(features_standardized[:,0].mean()))\n",
        "print(\"Standard deviation:\", features_standardized[:,0].std())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYVLybMe6Bre",
        "outputId": "62c3de31-b766-40a8-a782-1dfc2cf9d18c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean: 0\n",
            "Standard deviation: 0.9999999999999999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Libraries\n",
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "# Start Neural Network\n",
        "network = models.Sequential()\n",
        "\n",
        "# Add fully connected layer with a ReLU activation function\n",
        "network.add(layers.Dense(units=16, activation=\"relu\", input_shape= (10,)))\n",
        "\n",
        "network.add(layers.Dense(units=16, activation=\"relu\"))\n",
        "\n",
        "# Add fully connected layer with a sigmoid activation function\n",
        "network.add(layers.Dense(units=16, activation=\"sigmoid\"))\n",
        "\n",
        "# Compile Neural Network\n",
        "network.compile(loss=\"binary_crossentropy\", # Cross entropy\n",
        "                optimizer=\"rmsprop\",\n",
        "                metrics=[\"accuracy\"])\n"
      ],
      "metadata": {
        "id": "9J3kLXAq6KLr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load libraries\n",
        "import numpy as np\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "# Set random seed\n",
        "np.random.seed(0)\n",
        "\n",
        "# Set the number of features we want\n",
        "number_of_features = 1000\n",
        "\n",
        "# Load data and target vector from movie review data\n",
        "(data_train, target_train), (data_test, target_test) = imdb.load_data(\n",
        "    num_words=number_of_features)\n",
        "\n",
        "# Convert movie review data to one-hot encoded feature matrix\n",
        "tokenizer = Tokenizer(num_words=number_of_features)\n",
        "features_train = tokenizer.sequences_to_matrix(data_train, mode=\"binary\")\n",
        "features_test = tokenizer.sequences_to_matrix(data_test, mode=\"binary\")\n",
        "\n",
        "# Start neural network\n",
        "network = models.Sequential()\n",
        "\n",
        "# Add fully connected layer with a ReLU activation function\n",
        "network.add(layers.Dense(units=16, activation=\"relu\", input_shape=(\n",
        "    number_of_features,)))\n",
        "\n",
        "# Add fully connected layer with a ReLU activation function\n",
        "network.add(layers.Dense(units=16, activation=\"relu\"))\n",
        "\n",
        "# Add fully connected layer with a sigmoid activation function\n",
        "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
        "\n",
        "# Compile neural network\n",
        "network.compile(loss=\"binary_crossentropy\", # Cross-entropy\n",
        "                optimizer=\"rmsprop\", # Root Mean Square Propagation\n",
        "                metrics=[\"accuracy\"]) # Accuracy performance metric\n",
        "\n",
        "# Train neural network\n",
        "history = network.fit(features_train, # Features\n",
        "                      target_train, # Target vector\n",
        "                      epochs=3, # Number of epochs\n",
        "                      verbose=1, # Print description after each epoch\n",
        "                      batch_size=100, # Number of observations per batch\n",
        "                      validation_data=(features_test, target_test)) # Test data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DS_TBmyr7WIo",
        "outputId": "c4cddd1b-1372-4c5a-8704-31836ff4171d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "250/250 [==============================] - 2s 4ms/step - loss: 0.4162 - accuracy: 0.8144 - val_loss: 0.3407 - val_accuracy: 0.8538\n",
            "Epoch 2/3\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 0.3236 - accuracy: 0.8646 - val_loss: 0.3569 - val_accuracy: 0.8455\n",
            "Epoch 3/3\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 0.3142 - accuracy: 0.8676 - val_loss: 0.3283 - val_accuracy: 0.8581\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# View Shape of Feature Matrix\n",
        "features_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NPpRPfY7jFg",
        "outputId": "ac38d299-2b2e-4496-d4f6-d8afa547dbac"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 1000)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load libraries\n",
        "import numpy as np\n",
        "from keras.datasets import reuters\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "# Set random seed\n",
        "np.random.seed(0)\n",
        "\n",
        "# Set the number of features we want\n",
        "number_of_features = 5000\n",
        "\n",
        "# Load feature and target data\n",
        "data = reuters.load_data(num_words=number_of_features)\n",
        "(data_train, target_vector_train), (data_test, target_vector_test) = data\n",
        "\n",
        "# Convert feature data to a one-hot encoded feature matrix\n",
        "tokenizer = Tokenizer(num_words=number_of_features)\n",
        "features_train = tokenizer.sequences_to_matrix(data_train, mode=\"binary\")\n",
        "features_test = tokenizer.sequences_to_matrix(data_test, mode=\"binary\")\n",
        "\n",
        "# One-hot encode target vector to create a target matrix\n",
        "target_train = to_categorical(target_vector_train)\n",
        "target_test = to_categorical(target_vector_test)\n",
        "\n",
        "# Start neural network\n",
        "network = models.Sequential()\n",
        "\n",
        "# Add fully connected layer with a ReLU activation function\n",
        "network.add(layers.Dense(units=100,\n",
        "                         activation=\"relu\",\n",
        "                         input_shape=(number_of_features,)))\n",
        "\n",
        "# Add fully connected layer with a ReLU activation function\n",
        "network.add(layers.Dense(units=100, activation=\"relu\"))\n",
        "\n",
        "# Add fully connected layer with a softmax activation function\n",
        "network.add(layers.Dense(units=46, activation=\"softmax\"))\n",
        "\n",
        "# Compile neural network\n",
        "network.compile(loss=\"categorical_crossentropy\", # Cross-entropy\n",
        "                optimizer=\"rmsprop\", # Root Mean Square Propagation\n",
        "                metrics=[\"accuracy\"]) # Accuracy performance metric\n",
        "\n",
        "# Train neural network\n",
        "history = network.fit(features_train, # Features\n",
        "                      target_train, # Target\n",
        "                      epochs=3, # Three epochs\n",
        "                      verbose=0, # No output\n",
        "                      batch_size=100, # Number of observations per batch\n",
        "                      validation_data=(features_test, target_test)) # Test data"
      ],
      "metadata": {
        "id": "4lC9Ipwp7qiF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View Target Matrix\n",
        "target_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QcdUhm-77vok",
        "outputId": "9a39ae75-8983-4cb3-cfe4-1c3f91f741e2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load libraries\n",
        "import numpy as np\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "\n",
        "# Set random seed\n",
        "np.random.seed(0)\n",
        "\n",
        "# Generate features matrix and target vector\n",
        "features, target = make_regression(n_samples = 10000,\n",
        "                                   n_features = 3,\n",
        "                                   n_informative = 3,\n",
        "                                   n_targets = 1,\n",
        "                                   noise = 0.0,\n",
        "                                   random_state = 0)\n",
        "\n",
        "# Divide our data into training and test sets\n",
        "features_train, features_test, target_train, target_test = train_test_split(\n",
        "features, target, test_size=0.33, random_state=0)\n",
        "\n",
        "# Start neural network\n",
        "network = models.Sequential()\n",
        "\n",
        "# Add fully connected layer with a ReLU activation function\n",
        "network.add(layers.Dense(units=32,\n",
        "                         activation=\"relu\",\n",
        "                         input_shape=(features_train.shape[1],)))\n",
        "\n",
        "# Add fully connected layer with a ReLU activation function\n",
        "network.add(layers.Dense(units=32, activation=\"relu\"))\n",
        "\n",
        "# Add fully connected layer with no activation function\n",
        "network.add(layers.Dense(units=1))\n",
        "\n",
        "# Compile neural network\n",
        "network.compile(loss=\"mse\", # Mean squared error\n",
        "                optimizer=\"RMSprop\", # Optimization algorithm\n",
        "                metrics=[\"mse\"]) # Mean squared error\n",
        "\n",
        "# Train neural network\n",
        "history = network.fit(features_train, # Features\n",
        "                      target_train, # Target vector\n",
        "                      epochs=10, # Number of epochs\n",
        "                      verbose=0, # No output\n",
        "                      batch_size=100, # Number of observations per batch\n",
        "                      validation_data=(features_test, target_test)) # Test data"
      ],
      "metadata": {
        "id": "riZC_-XT78Ld"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load libraries\n",
        "import numpy as np\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "# Set random seed\n",
        "np.random.seed(0)\n",
        "\n",
        "# Set the number of features we want\n",
        "number_of_features = 10000\n",
        "\n",
        "# Load data and target vector from IMDB movie data\n",
        "(data_train, target_train), (data_test, target_test) = imdb.load_data(\n",
        "    num_words=number_of_features)\n",
        "\n",
        "# Convert IMDB data to a one-hot encoded feature matrix\n",
        "tokenizer = Tokenizer(num_words=number_of_features)\n",
        "features_train = tokenizer.sequences_to_matrix(data_train, mode=\"binary\")\n",
        "features_test = tokenizer.sequences_to_matrix(data_test, mode=\"binary\")\n",
        "\n",
        "# Start neural network\n",
        "network = models.Sequential()\n",
        "\n",
        "# Add fully connected layer with a ReLU activation function\n",
        "network.add(layers.Dense(units=16,\n",
        "                         activation=\"relu\",\n",
        "                         input_shape=(number_of_features,)))\n",
        "\n",
        "# Add fully connected layer with a ReLU activation function\n",
        "network.add(layers.Dense(units=16, activation=\"relu\"))\n",
        "\n",
        "# Add fully connected layer with a sigmoid activation function\n",
        "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
        "\n",
        "# Compile neural network\n",
        "network.compile(loss=\"binary_crossentropy\", # Cross-entropy\n",
        "                optimizer=\"rmsprop\", # Root Mean Square Propagation\n",
        "                metrics=[\"accuracy\"]) # Accuracy performance metric\n",
        "\n",
        "# Train neural network\n",
        "history = network.fit(features_train, # Features\n",
        "                      target_train, # Target vector\n",
        "                      epochs=3, # Number of epochs\n",
        "                      verbose=0, # No output\n",
        "                      batch_size=100, # Number of observations per batch\n",
        "                      validation_data=(features_test, target_test)) # Test data\n",
        "\n",
        "# Predict classes of test set\n",
        "predicted_target = network.predict(features_test)"
      ],
      "metadata": {
        "id": "SIn0FmzU8Bsp"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View the probability the first observation is class 1\n",
        "predicted_target[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgtZ6juE8FRD",
        "outputId": "44f8f8ad-e654-407b-dfbb-0391b4c50662"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.0913966], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ]
}