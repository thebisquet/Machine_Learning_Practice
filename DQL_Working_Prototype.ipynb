{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f26369c-5f65-46cf-ba7b-93c02af92b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install gym==0.15.3\n",
    "#!pip3 install atari_py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20ddeca6-781a-4c29-b2b6-dbace33426f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment creation and random number generator\n",
    "import gym, random, sys, copy, time, torch\n",
    "import math, glob, io, matplotlib, os\n",
    "import cv2, base64, keras\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "# Linear algebra and data manipulation libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from gym import logger as gymlogger\n",
    "from gym.wrappers import Monitor\n",
    "from gym.spaces import Box\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "\n",
    "# Deep learning model requirements\n",
    "import tensorflow.keras as keras\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87ca70aa-5348-4f3a-bfdd-08152b58dc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "836fdd5a-c703-4dbf-82b0-12c7ea9c9a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Q Network Model\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, learning_rate):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 8, stride=4, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3)\n",
    "        \n",
    "        self.fc1 = nn.Linear(128*19*8, 512)\n",
    "        self.fc2 = nn.Linear(512, 6) # 6 Actions\n",
    "        \n",
    "        self.optimizer = optim.RMSprop(self.parameters(), lr=learning_rate)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "        \n",
    "    def forward(self, observation):\n",
    "        # Convert frames to Tensor\n",
    "        obs = np.stack(observation, axis=0)\n",
    "        obs = T.Tensor(observation).to(self.device)\n",
    "        # Reshape for convolutional layers\n",
    "        obs = obs.view(-1, 1, 185, 95) \n",
    "        obs = F.relu(self.conv1(obs))\n",
    "        obs = F.relu(self.conv2(obs))\n",
    "        obs = F.relu(self.conv3(obs))\n",
    "        \n",
    "        # Flatten convolutional images, then feed into fc\n",
    "        obs = obs.view(-1, 128*19*8)\n",
    "        obs = F.relu(self.fc1(obs))\n",
    "        \n",
    "        actions = self.fc2(obs)\n",
    "        \n",
    "        # Returns a matrix, kx6 where k is number of images\n",
    "        return actions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "953fa5aa-824c-41c2-b259-ac26c0ee9302",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, gamma, epsilon, learning_rate, max_memory, epsilon_min=0.5, replace=10000, action_space=[0,1,2,3,4,5]):\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.action_space = action_space\n",
    "        self.max_memory = max_memory\n",
    "        self.steps = 0\n",
    "        self.step_counter = 0 # Target Network Replacement\n",
    "        self.memory = []\n",
    "        self.memory_counter = 0\n",
    "        self.replace_target_counter = replace\n",
    "        self.Q_eval = DQN(learning_rate) # Current state guess\n",
    "        self.Q_next = DQN(learning_rate) # Next state guess\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state):\n",
    "        if self.memory_counter < self.max_memory:\n",
    "            self.memory.append([state, action, reward, next_state])\n",
    "        else:\n",
    "            self.memory[self.memory_counter % self.max_memory] = [state, action, reward, next_state]\n",
    "        \n",
    "        self.memory_counter += 1\n",
    "    \n",
    "    def act(self, observation):\n",
    "        # Take in a sequence of observations\n",
    "        rand = np.random.random()\n",
    "        actions = self.Q_eval.forward(observation)\n",
    "        \n",
    "        if rand < 1 - self.epsilon:\n",
    "            action = T.argmax(actions[1]).item()\n",
    "        else:\n",
    "            action = np.random.choice(self.action_space)\n",
    "            \n",
    "        self.steps += 1\n",
    "        return action\n",
    "\n",
    "    def train(self, batch_size):\n",
    "        # Batch Learning Zero Grad\n",
    "        self.Q_eval.optimizer.zero_grad()\n",
    "        if self.replace_target_counter is not None and self.step_counter % self.replace_target_counter == 0:\n",
    "            self.Q_next.load_state_dict(self.Q_eval.state.dict())\n",
    "            \n",
    "        if self.memory_counter + batch_size < self.max_memory:\n",
    "            memory_start = int(np.random.choice(range(self.memory_counter)))\n",
    "        else:\n",
    "            memory_start = int(np.random.choice(range(self.max_memory - batch_size - 1)))\n",
    "        \n",
    "        mini_batch = self.memory[memory_start:memory_start + batch_size]\n",
    "        memory = np.array(mini_batch)\n",
    "        \n",
    "        Q_pred = self.Q_eval.forward(list(memory[:, 0][:])).to(self.Q_eval.device)\n",
    "        Q_next = self.Q_next.forward(list(memory[:, 3][:])).to(self.Q_eval.device)\n",
    "        \n",
    "        max_a = T.argmax(Q_next, dim=1).to(self.Q_eval.device)\n",
    "        rewards = T.Tensor(list(memory[:, 2])).to(self.Q_eval.device)\n",
    "        Q_target = Q_pred\n",
    "        Q_target[:, max_a] = rewards + self.gamma * T.max(Q_next[1])\n",
    "        \n",
    "        if self.steps > 500:\n",
    "            if self.epsilon - 1e-4 > self.epsilon_min:\n",
    "                self.epsilon -= 1e-4 # Converge Epsilon\n",
    "            else:\n",
    "                self.epsilon = self.epsilon_min\n",
    "                \n",
    "        loss = self.Q_eval.loss(Q_target, Q_pred).to(self.Q_eval.device)\n",
    "        loss.backward()\n",
    "        self.Q_eval.optimizer.step()\n",
    "        self.step_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8801771f-e455-4f36-91ae-963cd29a3c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Initializing Agent Memory\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('SpaceInvaders-v4')\n",
    "agent = Agent(gamma=0.95, epsilon=1.0, learning_rate=0.03, max_memory=5000, replace=None)\n",
    "\n",
    "# Initialize our agents memory\n",
    "while agent.memory_counter < agent.max_memory:\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done and info['ale.lives'] == 0:\n",
    "            reward = -100\n",
    "        \n",
    "        agent.remember(np.mean(observation[15:200, 30:125], axis=2),\n",
    "                               action, reward, np.mean(state[15:200, 30:125], axis=2))\n",
    "        observation = state\n",
    "        \n",
    "print(\"Finished Initializing Agent Memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825b5ae2-3896-49bd-a6fb-835bb8807b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game 1 hyperparameters: Epsilon: 1.0 Average Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "avg_score = 0\n",
    "episode_history = []\n",
    "episodes = 50\n",
    "batch_size = 32\n",
    "total_runs = 1\n",
    "\n",
    "# Record Gameplay\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()\n",
    "env = Monitor(env, './video', force=True)\n",
    "\n",
    "for episode in range(episodes):\n",
    "\n",
    "    print(\"Game {} hyperparameters: Epsilon: {} Average Score: {}\".format(episode+1, agent.epsilon, (avg_score/total_runs)))\n",
    "    episode_history.append(agent.epsilon)\n",
    "    done = False\n",
    "    observation = env.reset()\n",
    "    frames = [np.sum(observation[15:200, 30:125], axis=2)]\n",
    "    score = 0\n",
    "    last_action = 0\n",
    "    \n",
    "    while not done:\n",
    "        if len(frames) == 3:\n",
    "            action = agent.act(frames)\n",
    "            frames = []\n",
    "        else:\n",
    "            action = last_action\n",
    "            \n",
    "        state, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "        frames.append(np.sum(observation[15:200, 30:125], axis=2))\n",
    "        if done and info['ale.lives'] == 0:\n",
    "            reward = -100\n",
    "            \n",
    "        agent.remember(np.mean(observation[15:200, 30:125], axis=2), action, reward, np.mean(state[15:200, 30:125], axis=2))\n",
    "    \n",
    "        observation = state\n",
    "        agent.train(batch_size)\n",
    "        last_action = action\n",
    "                       \n",
    "    scores.append(score)\n",
    "    avg_score += score\n",
    "    total_runs += 1\n",
    "    print(\"End Score: {}\".format(score))\n",
    "    \n",
    "    # Render a video of the session\n",
    "    mp4list = glob.glob('video/*.mp4')\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = mp4list[-1]\n",
    "        video = io.open(mp4, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        ipythondisplay.display(HTML(data='''<video alt='test' autoplay loop controls style='height: 400px;'> <source src='data:video/mp4;base64,{0}' type='video/mp4' /> </video>'''.format(encoded.decode('ascii'))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3effb4c2-86b2-4250-bb8b-f0a3d9ca905e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
